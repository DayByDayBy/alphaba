{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a697c6ff-f527-4f7f-afba-3e7617e8669b",
   "metadata": {},
   "source": [
    "# some more analysis!\n",
    "\n",
    "this time i am looking at variance within, as i am curious whether 'trust the neurons' is sufficient for retaining upper and lower case letter similarity (my concern is it won't, and might make a 52 letter alphabet rather than 26 paired upper/lower letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffec3d6-fc23-4475-b3b9-ec87e1aa7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1bf0387-7ec3-4498-acd9-1707fd2f5b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 alphabets\n",
      "  Gujarati: 960 images\n",
      "  Korean: 800 images\n",
      "  Arcadian: 520 images\n",
      "  Malay_(Jawi_-_Arabic): 800 images\n",
      "  Grantha: 860 images\n",
      "  Blackfoot_(Canadian_Aboriginal_Syllabics): 280 images\n",
      "  Balinese: 480 images\n",
      "  Futurama: 520 images\n",
      "  N_Ko: 660 images\n",
      "  Burmese_(Myanmar): 680 images\n",
      "  Anglo-Saxon_Futhorc: 580 images\n",
      "  Mkhedruli_(Georgian): 820 images\n",
      "  Latin: 520 images\n",
      "  Braille: 520 images\n",
      "  Sanskrit: 840 images\n",
      "  Japanese_(hiragana): 1040 images\n",
      "  Tagalog: 340 images\n",
      "  Greek: 480 images\n",
      "  Ojibwe_(Canadian_Aboriginal_Syllabics): 280 images\n",
      "  Japanese_(katakana): 940 images\n",
      "  Early_Aramaic: 440 images\n",
      "  Hebrew: 440 images\n",
      "  Tifinagh: 1100 images\n",
      "  Asomtavruli_(Georgian): 800 images\n",
      "  Armenian: 820 images\n",
      "  Syriac_(Estrangelo): 460 images\n",
      "  Alphabet_of_the_Magi: 400 images\n",
      "  Cyrillic: 660 images\n",
      "  Bengali: 920 images\n",
      "  Inuktitut_(Canadian_Aboriginal_Syllabics): 320 images\n",
      "Loaded 30 alphabets\n",
      "  Gujarati: 960 images\n",
      "  Korean: 800 images\n",
      "  Arcadian: 520 images\n",
      "  Malay_(Jawi_-_Arabic): 800 images\n",
      "  Grantha: 860 images\n",
      "  Blackfoot_(Canadian_Aboriginal_Syllabics): 280 images\n",
      "  Balinese: 480 images\n",
      "  Futurama: 520 images\n",
      "  N_Ko: 660 images\n",
      "  Burmese_(Myanmar): 680 images\n",
      "  Anglo-Saxon_Futhorc: 580 images\n",
      "  Mkhedruli_(Georgian): 820 images\n",
      "  Latin: 520 images\n",
      "  Braille: 520 images\n",
      "  Sanskrit: 840 images\n",
      "  Japanese_(hiragana): 1040 images\n",
      "  Tagalog: 340 images\n",
      "  Greek: 480 images\n",
      "  Ojibwe_(Canadian_Aboriginal_Syllabics): 280 images\n",
      "  Japanese_(katakana): 940 images\n",
      "  Early_Aramaic: 440 images\n",
      "  Hebrew: 440 images\n",
      "  Tifinagh: 1100 images\n",
      "  Asomtavruli_(Georgian): 800 images\n",
      "  Armenian: 820 images\n",
      "  Syriac_(Estrangelo): 460 images\n",
      "  Alphabet_of_the_Magi: 400 images\n",
      "  Cyrillic: 660 images\n",
      "  Bengali: 920 images\n",
      "  Inuktitut_(Canadian_Aboriginal_Syllabics): 320 images\n",
      "Training model...\n",
      "Epoch 1/3\n",
      " Step 0/20 - loss: 23.1464\n",
      "Debug - Checking if model weights change...\n",
      " Epoch 1 - avg loss: 561.0146\n",
      "--------------------------------------------------\n",
      "Epoch 2/3\n",
      " Step 0/20 - loss: 543.3092\n",
      " Epoch 2 - avg loss: 604.1309\n",
      "--------------------------------------------------\n",
      "Epoch 3/3\n",
      " Step 0/20 - loss: 442.9082\n",
      " Epoch 3 - avg loss: 431.1173\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously create file (unable to open file: name = 'models/triplet.weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     10\u001b[39m history = train_triplet_model_custom(\n\u001b[32m     11\u001b[39m     triplet_model, \n\u001b[32m     12\u001b[39m     loader, \n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     steps_per_epoch=\u001b[32m20\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# save weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtriplet_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/triplet.weights.h5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# run analysis\u001b[39;00m\n\u001b[32m     24\u001b[39m results = run_case_analysis(base_network, loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build_space/alph/alphaba/.venv/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build_space/alph/alphaba/.venv/lib/python3.13/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n\u001b[32m    560\u001b[39m                      **kwds)\n\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build_space/alph/alphaba/.venv/lib/python3.13/site-packages/h5py/_hl/files.py:244\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n\u001b[32m    242\u001b[39m     fid = h5f.create(name, h5f.ACC_EXCL, fapl=fapl, fcpl=fcpl)\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mACC_TRUNC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    246\u001b[39m     \u001b[38;5;66;03m# Open in append mode (read/write).\u001b[39;00m\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:122\u001b[39m, in \u001b[36mh5py.h5f.create\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Unable to synchronously create file (unable to open file: name = 'models/triplet.weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)"
     ]
    }
   ],
   "source": [
    "from src.analysis_tools import *\n",
    "from src.training import train_triplet_model_custom\n",
    "from src.data_loader import OmniglotTripletLoader\n",
    "\n",
    "loader = OmniglotTripletLoader(\"../../omniglot/python\")\n",
    "triplet_model, base_network = create_triplet_model(embedding_dim=128)\n",
    "\n",
    "# train model (quick training for testing, 3 epochs)\n",
    "print(\"Training model...\")\n",
    "history = train_triplet_model_custom(\n",
    "    triplet_model, \n",
    "    loader, \n",
    "    epochs=3, \n",
    "    batch_size=16, \n",
    "    steps_per_epoch=20\n",
    ")\n",
    "\n",
    "# save weights\n",
    "triplet_model.save_weights(\"models/triplet.weights.h5\")\n",
    "\n",
    "\n",
    "\n",
    "# run analysis\n",
    "results = run_case_analysis(base_network, loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488b6be-511c-4b12-a640-f3e31303ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysis complete!\")\n",
    "print(f\"Found {len(results['embeddings'])} total embeddings\")\n",
    "print(f\"Top similar cross-alphabet pair: {results['top_cross_pairs'][0] if results['top_cross_pairs'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9511d-59b2-4b2c-bfc8-ab57cec6fc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997b25d-74ce-43e0-81bb-6e5c21995192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
