{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a697c6ff-f527-4f7f-afba-3e7617e8669b",
   "metadata": {},
   "source": [
    "# some more analysis!\n",
    "\n",
    "this time i am looking at variance within, as i am curious whether 'trust the neurons' is sufficient for retaining upper and lower case letter similarity (my concern is it won't, and might make a 52 letter alphabet rather than 26 paired upper/lower letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffec3d6-fc23-4475-b3b9-ec87e1aa7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1bf0387-7ec3-4498-acd9-1707fd2f5b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 alphabets\n",
      "  Gujarati: 960 images\n",
      "  Korean: 800 images\n",
      "  Arcadian: 520 images\n",
      "  Malay_(Jawi_-_Arabic): 800 images\n",
      "  Grantha: 860 images\n",
      "  Blackfoot_(Canadian_Aboriginal_Syllabics): 280 images\n",
      "  Balinese: 480 images\n",
      "  Futurama: 520 images\n",
      "  N_Ko: 660 images\n",
      "  Burmese_(Myanmar): 680 images\n",
      "  Anglo-Saxon_Futhorc: 580 images\n",
      "  Mkhedruli_(Georgian): 820 images\n",
      "  Latin: 520 images\n",
      "  Braille: 520 images\n",
      "  Sanskrit: 840 images\n",
      "  Japanese_(hiragana): 1040 images\n",
      "  Tagalog: 340 images\n",
      "  Greek: 480 images\n",
      "  Ojibwe_(Canadian_Aboriginal_Syllabics): 280 images\n",
      "  Japanese_(katakana): 940 images\n",
      "  Early_Aramaic: 440 images\n",
      "  Hebrew: 440 images\n",
      "  Tifinagh: 1100 images\n",
      "  Asomtavruli_(Georgian): 800 images\n",
      "  Armenian: 820 images\n",
      "  Syriac_(Estrangelo): 460 images\n",
      "  Alphabet_of_the_Magi: 400 images\n",
      "  Cyrillic: 660 images\n",
      "  Bengali: 920 images\n",
      "  Inuktitut_(Canadian_Aboriginal_Syllabics): 320 images\n",
      "Training model...\n",
      "Epoch 1/3\n",
      " Step 0/20 - loss: 32.2095\n",
      "Debug - Checking if model weights change...\n",
      " Epoch 1 - avg loss: 609.8336\n",
      "--------------------------------------------------\n",
      "Epoch 2/3\n",
      " Step 0/20 - loss: 180.8777\n",
      " Epoch 2 - avg loss: 531.2462\n",
      "--------------------------------------------------\n",
      "Epoch 3/3\n",
      " Step 0/20 - loss: 273.8881\n",
      " Epoch 3 - avg loss: 383.0990\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filename must end in `.weights.h5`. Received: filepath=models/triplet_weights.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      9\u001b[39m history = train_triplet_model_custom(\n\u001b[32m     10\u001b[39m     triplet_model, \n\u001b[32m     11\u001b[39m     loader, \n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     steps_per_epoch=\u001b[32m20\u001b[39m\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Save the weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mtriplet_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/triplet_weights.h5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Now run analysis\u001b[39;00m\n\u001b[32m     21\u001b[39m results = run_case_analysis(base_network, loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build_space/alph/alphaba/.venv/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build_space/alph/alphaba/.venv/lib/python3.13/site-packages/keras/src/saving/saving_api.py:230\u001b[39m, in \u001b[36msave_weights\u001b[39m\u001b[34m(model, filepath, overwrite, max_shard_size, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m filepath_str = \u001b[38;5;28mstr\u001b[39m(filepath)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filepath_str.endswith(\u001b[33m\"\u001b[39m\u001b[33m.weights.h5\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    231\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe filename must end in `.weights.h5`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    232\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    233\u001b[39m     )\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filepath_str.endswith(\n\u001b[32m    235\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mweights.h5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mweights.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    236\u001b[39m ):\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    238\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe filename must end in `.weights.json` when `max_shard_size` is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mspecified. Received: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    240\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The filename must end in `.weights.h5`. Received: filepath=models/triplet_weights.h5"
     ]
    }
   ],
   "source": [
    "from src.analysis_tools import *\n",
    "from src.training import train_triplet_model_custom\n",
    "\n",
    "loader = OmniglotTripletLoader(\"../../omniglot/python\")\n",
    "triplet_model, base_network = create_triplet_model(embedding_dim=128)\n",
    "\n",
    "# train model (quick training for testing, 3 epochs)\n",
    "print(\"Training model...\")\n",
    "history = train_triplet_model_custom(\n",
    "    triplet_model, \n",
    "    loader, \n",
    "    epochs=3, \n",
    "    batch_size=16, \n",
    "    steps_per_epoch=20\n",
    ")\n",
    "\n",
    "# save weights\n",
    "triplet_model.save_weights(\"models/triplet.weights.h5\")\n",
    "\n",
    "# run analysis\n",
    "results = run_case_analysis(base_network, loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488b6be-511c-4b12-a640-f3e31303ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysis complete!\")\n",
    "print(f\"Found {len(results['embeddings'])} total embeddings\")\n",
    "print(f\"Top similar cross-alphabet pair: {results['top_cross_pairs'][0] if results['top_cross_pairs'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9511d-59b2-4b2c-bfc8-ab57cec6fc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997b25d-74ce-43e0-81bb-6e5c21995192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
